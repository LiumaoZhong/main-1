\chapter{Syntax as First-Order Logic}
\label{cha:FO}

We are now operating under the assumption that syntax can be adequately described in terms of regular tree languages.
They are sufficiently expressive to capture advanced linguistic concepts like headedness, projection, subcategorization, agreement, adjunction, and instances of displacement like wh-movement and topicalization in English.
At the same time, they can be automatically converted into CFGs, which makes them suitable for a variety of well-understood parsing techniques.
Today we will look once more at what can and cannot be done with regular tree languages, and we will see that, puzzlingly, syntax is both within and outside this class, depending on what kind of object we take syntax to define.

\section{Syntactic Constraints}

\subsection{A General Template for Constraints}
The syntactic literature is full of an enormous number of constraints.
Some examples include:
%
\begin{itemize}
    \item the Empty Category Principle (ECP), which restricts the possible landing sites of movement,
    \item Binding theory, which restricts the distribution of pronouns (\emph{him}, \emph{her}) and reflexives (\emph{himself}, \emph{herself}),
    \item the Person Case Constraint (PCC), which blocks certain combinations of pronouns in a sentence,
    \item the Shortest Derivation Principle (SDP), which favors shorter derivations over longer ones.
\end{itemize}
%
This is obviously not a complete list, and linguists keep coming up with new constraints or modify old ones to better fit the data.
How could we possibly say that all constraints in natural language syntax fall within the class of regular tree languages?

As with any other empirical science, there is of course the possibility that a new discovery completely contradicts our current theories and requires new, more powerful machinery.
But with any new piece of data we discover that matches current proposals, this scenario becomes less and less likely.
After 50 years of generative linguistics with many competing formalisms, an abstract template for the formulation of constraints has crystallized.
Constraints that are stated with respect to a single tree usually involves four components:
%
\begin{itemize}
    \item a finite number of nodes between which the dependency holds (e.g.\ the target site and the source of a moving phrase),
    \item a tree-geometric relation that picks out these nodes (e.g.\ c-command)
    \item a locality domain within which the dependency must be satisfied,
    \item a logical control mechanism that triggers the dependency (e.g.\ ``if X is in position Y, then Z must be satisfied'')
\end{itemize}
%
Comparative constraints like the SDP, where the well-formedness of a tree cannot be decided without looking at other trees, do not obviously fit this template, and we will not discuss them here.
However, \citet{Graf13Thesis} shows that they, too, can be broken down into templates of this form (using ideas we encountered in the proof that certain variants of OT generate regular string languages).

The existence of such a template for constraints is crucial because if all four factors stay within the realm of regular tree languages, then all constraints that obey this template are regular, too.
In the early 90s it was realized that constraints obeying the template can be easily expressed as statements of a formal description language that can be automatically translated into refined strictly $2$-local tree grammars, thereby establishing their regularity.
This insight forms the backbone of what is now known as \emph{model-theoretic syntax} \citep{Blackburn.etal93,Backofen.etal95,Kracht97,Rogers98,PottsPullum02,Pullum07}.

\subsection{Constraints, Logics, and Model Theory}

If one abstracts away from all matters of implementation, a constraint $c$ over trees is simply a method for defining the largest set of trees that satisfy $c$, or equivalently, do not violate $c$.
So we can equate every constraint with a (possibly infinite) set of trees.

Something remarkably similar can be found in mathematical logic.
There are many different logics, e.g.\ propositional logic, first-order logic, or modal logic.
What makes each one of them a logic is that they are formal systems with a well-defined syntax, which defines what strings are well-formed formulas of the logic, and a semantics that assigns a specific interpretation to each formula.

For first-order logic, the syntax can be defined in a way that's very close to a context-free grammar.
First, we have to fix a vocabulary, also called a \emph{signature}.
It includes a finite number of relational symbols $R_i^n$ of arity $n$, and a set $O$ of logical operands:
%
\begin{center}
    \begin{tabular}{cc}
        $\wedge$ & and\\
        $\vee$ & or\\
        $\rightarrow$ & implies\\
        $\leftrightarrow$ & if and only if\\
        $\neg$ & not\\
        $\forall$ & for every\\
        $\exists$ & there exists
    \end{tabular}
\end{center}
%
In addition, there is an infinite set of variables $x_1, \ldots, x_n, \ldots$, and the bracketing symbols ``('' and ``)''.
The set of well-formed formulas is given by the following rules:
%
\begin{center}
    \begin{tabular}{rcl}
        $S$ & \rewrite & $(S \wedge S)$\\
        $S$ & \rewrite & $(S \vee S)$\\
        $S$ & \rewrite & $(S \rightarrow S)$\\
        $S$ & \rewrite & $(S \leftrightarrow S)$\\
        $S$ & \rewrite & $(\neg S)$\\
        $S$ & \rewrite & $(\forall x_i\ S)$\\
        $S$ & \rewrite & $(\exists x_i\ S)$\\
        $S$ & \rewrite & $R_i^n(x_1,\ldots,x_n)$
    \end{tabular}
\end{center}
%
Assuming that our only relational symbols are binary $\idom$ and unary $\mathrm{CP}$, the rules above tell us that the following is a well-formed formula:
%
\[
    (\forall x (
        (\neg (\exists y (\idom(y,x))))
            \rightarrow
        \mathrm{CP}(x)
        )
\]
%
Assuming certain standard conventions about operator scope switching to infix notation for binary relations, we can drop some of the brackets and simplify the formula to
%
\(
    \forall x (
        \neg \exists y (y \idom x)
            \rightarrow
        \mathrm{CP}(x)
        )
\), and we can also represent it in terms of a tree (the left format shows the phrase structure, the right one the dependency structure).
% fixme: if we had discussed dependency structure earlier on, this would be more natural, and we would already have the notion of a more complex yield function because dependency trees do not directly encode string yield
%
\begin{center}
    \begin{forest}
        [S
            [$\forall x$]
            [S
                [S
                    [$\neg$]
                    [S
                        [$\exists y$]
                        [S
                            [$y \idom x$]
                        ]
                    ]
                ]
                [$\rightarrow$]
                [S
                    [$\mathrm{CP}(x)$]
                ]
            ]
        ]
    \end{forest}
    %
    \hspace{2em}
    %
    \begin{forest}
        [$\forall x$
            [$\rightarrow$
                [$\neg$
                    [$\exists y$
                        [$y \idom x$]
                    ]
                ]
                [$\mathrm{CP}(x)$]
            ]
        ]
    \end{forest}
\end{center}

Defining the set of well-formed formulas gives us only one half of a logic, the much more important half is to endow each well-formed formula with a specific meaning.
This is achieved by the notion of a \emph{model}.
A mathematical structure $M$ is a model of formula $\phi$ (written $M \models \phi$) iff $\phi$ holds in $M$ under a fixed interpretation.
This interpretation is defined in a piece-wise fashion, where $\phi[x \leftarrow u]$ is the result of replacing every free instance of $x$ (= not c-commanded\slash dominated by $\forall x$ or $\exists x$ in the phrase structure\slash dependency tree) in $\phi$ by $u$.
%fixme: explain free variable notion via c-command
%
\begin{center}
    \begin{tabular}{ll}
        $M \models \phi \wedge \psi$          & iff both $M \models \phi$ and $M \models \psi$\\
        $M \models \phi \vee \psi$            & iff $M \models \phi$ or $M \models \psi$\\
        $M \models \phi \rightarrow \psi$     & iff $M \models \phi$ implies $M \models \psi$\\
        $M \models \phi \leftrightarrow \psi$ & iff $M \models \phi$ implies $M \models \psi$ and the other way round\\
        $M \models \neg \phi$                 & iff $M \models \phi$ does not hold\\
        $M \models \forall x \phi$            & iff $M \models \phi[x \leftarrow u]$ for all $u$ of $M$\\
        $M \models \exists x \phi$            & iff $M \models \phi[x \leftarrow u]$ for some $u$ of $M$\\
    \end{tabular}
\end{center}
%
In addition, one must also define an interpretation for all relational symbols.
For example, if we assume that our class of models is the set of all trees, we may take $\idom$ to denote the mother-of relation, whereas $\mathrm{CP}(x)$ means that node $x$ has the label CP\@.
In that case, the formula above holds of all trees whose root is labeled CP\@.
Or the other way round, every tree whose root is labeled CP is a model for this formula.

Just like every constraint can be identified with the set of trees that satisfy it, every formula can be identified with its set of models.
But this means that we can actually think of constraints in terms of logical formulas: a constraint $c$ is a logical formula $\phi$ such that the set of structures obeying $c$ is exactly the set of models of $\phi$.
%
\begin{examplebox}[A Formula for Trace Licensing]
    The ECP states that every trace must be c-commanded by the moving phrase that left behind this trace.
    We will formalize a simplified variant that ensures the presence of a c-commanding phrase that could be the original mover.
    To this end, we use first-order logic with a signature that contains $\pdom$ for proper dominance, $\approx$ for equivalence, and a unary predicate for every label in our intended tree alphabet.
    Note that we do not have to add the mother-of relation to the signature because it can be defined in terms of proper dominance:
    %
    \[
        x \idom y \iff
            x \pdom y
            \wedge
            \neg \exists z
                [x \pdom z \wedge z \pdom y]
    \]
    % 
    The same is true of c-command: 
    %
    \[
        \text{c-com}(x,y) \iff
            \neg (x \rdom y)
            \wedge
            \neg (y \rdom x)
            \wedge
            \forall z [z \pdom x \rightarrow z \pdom y] 
    \]
    %
    Here $\rdom$ denotes reflexive dominance, which is also definable from proper dominance and equivalence.
    %
    \[
        x \rdom y \iff
            x \approx y
            \vee
            x \pdom y
    \]
    %
    So now we have all the predicates we need to talk about the tree geometric configuration between traces and movers, but we still need a way to identify potential movers.

    Clearly a phrase is a mover if it occupies a position where it cannot have been selected as an argument.
    According to the standard theory of movement, movers can only occur in specifiers, which are usually reserved for the second argument of a head.
    So it suffices to look at the head of the phrase and check whether it selects a second argument (for the sake of simplicity, we will assume that no head has an optional second argument, that phrases obey a strict X$'$-template where every phrase has at most one specifier, and we also ignore adjuncts, which would be identified as potential movers with this approach).

    \clearpage
    First we need to define a predicate that identifies whether a node is a specifier.
    A phrase is a specifier iff it is the sibling of a node labeled X$'$.
    Rather than assuming that the trees include this information in their interior node labels, we define a predicate for identifying the X$'$-node projected by a lexical item, which is then referenced in the definition of the predicate for specifiers.
    %
    \begin{align*}
        \mathrm{bar}(x,y) & \iff 
            \exists z [
                x \idom z
                \wedge
                z \idom y
                \wedge
                \bigvee_{l \text{ a lexical item}} l(y)
                ]\\
                %
        \mathrm{sibling}(x,y) & \iff
            \exists z [
                z \idom x
                \wedge
                z \idom y
                ]\\
                %
        \mathrm{spec}(x,y) & \iff
            \exists z [
                \mathrm{sibling}(x,z)
                \wedge
                \mathrm{bar}(z,y)
                ]
    \end{align*}
    %
    Now we can finally define a predicate to identify moving phrases as specifiers of a head that takes no second argument.
    %
    \[
        \mathrm{mover}(x) \iff
            \exists y [
                \mathrm{spec}(x,y)
                \wedge
                \bigvee_{l \text{ takes at most one argument}} l(y)
            ]
    \]

    In the last step we require every trace to be c-commanded by a phrase.
    %
    \[
        \forall x [
            t(x) \rightarrow
                \exists y
                    [
                    \mathrm{mover}(y)
                    \wedge
                    \text{c-com}(y,x)
                ]
        ]
    \]
    %
    Note that this is an actual formula, whereas all the previous formulas are just definitions that define predicates as a shorthand for specific subformulas that we use in definition of the ECP constraint.

    Our implementation of the ECP leaves a lot to be desired, of course (a more adequate implementation is given in \citealt{Rogers98}, a logical formalization of GB that takes up over 150 pages).
    One glaring omission is that a mover cannot have left a specific trace behind if its category feature does not fit the position in which the trace occurs, for in that case the mover cannot have originated there.
    This can be added to the formula above by defining a predicate that checks what type of category is required for the position occupied by a trace and checks that the mover has this category --- a rather simple exercise.
    A more interesting aspect is that movement is usually assumed to be locally bounded, which means that the licensor of a trace must occur within a specific domain, e.g.\ the smallest CP containing the trace.
    Such domain restrictions are very common across linguistic constraints, but fortunately they pose no challenge to first-order logic.
    %
    \begin{align*}
        \mathrm{smallest}(x,y,\mathit{ZP}) & \iff
                \mathit{ZP}(y)
                \wedge
                y \pdom x
                \wedge
                \neg \exists z
                    [
                    \mathit{ZP}(z)
                    \wedge
                    y \pdom z
                    \wedge
                    z \pdom x
                    ]
                \\
        \mathrm{local}(x,y,\mathit{ZP}) & \iff
            \exists z
                [
                \mathrm{smallest}(x,z,\mathit{ZP})
                \wedge
                \mathrm{smallest}(y,z,\mathit{ZP})
                ]
    \end{align*}
    %
    \[
        \forall x [
            t(x) \rightarrow
                \exists y
                    [
                    \mathrm{mover}(y)
                    \wedge
                    \text{c-com}(y,x)
                    \wedge
                    \mathrm{local}(x,y,\mathrm{CP})
                ]
        ]
    \]
\end{examplebox}

While the example above implements a specific constraint, it is easy to see that the tricks and techniques work for virtually all linguistic constraints.
Lexicalization and projection means that all local information can be inferred from the head of a phrase, and we can keep track of this information via newly defined predicates.
More complex tree geometric notions like c-command or m-command are easily defined in terms of dominance, and the same holds for the notions of closeness and locality domain that are ubiquitous in linguistics.
With a little bit of ingenuity, pretty much all syntactic constraints can be expressed in first-order logic.

First-order logic is a fragment of monadic second-order logic (MSO), which enriches first-order logic with the option to quantify over sets of nodes.
It turns out that MSO is exactly as powerful as refined strictly $2$-local tree grammars, so one can freely translate between grammars and MSO formulas \citep{Buechi60}.
The construction is fairly involved (see \citealt{Morawietz03} for details), and the computational complexity of the translation is unbounded --- it doesn't get any less tractable than that.
Nonetheless the conversion is reasonably fast in practice, and the fact that linguistic constraints can be stated without set quantification reduces the complexity a lot.
Overall, then, it is a viable strategy to formalize linguistic theories in terms of first-order formulas, which are then automatically translated into refined tree grammars or possibly even CFGs.
%
\begin{conjecture}[FO-Syntax]
    All aspects of natural language syntax can be expressed in terms of first-order formulas over trees without interior node labels.
\end{conjecture}

\section{Pushing the Limits}

\subsection{Polarity Items}

There are a few phenomena that push the limits of what can be expressed in first-order logic.
One of them is the distribution of so-called \emph{polarity items}, or at least their predicted distribution according to standard generalizations.
Polarity items can only occur in a context of a specific polarity.
Negative polarity items (NPIs) are restricted to sentences where they appear in the scope of some negative element, though it is difficult to characterize what counts as such a negative element, as the examples below illustrate for the idiomatic NPI \emph{lift a finger}:
%
\begin{exe}
    \ex
    \begin{xlist}
        \ex[]  {Nobody did so much as lift a finger.}
        \ex[*] {Everybody did so much as lift a finger.}
        \ex[]  {You didn't so much as lift a finger.}
        \ex[*] {You did so much as lift a finger.}
        \ex[]  {You never did so much as lift a finger.}
        \ex[]  {You hardly did so much as lift a finger.}
        \ex[*] {You always did so much as lift a finger.}
    \end{xlist}
\end{exe}
%
Negation and negative quantifiers license this NPI, but so do adverbials like \emph{never} and \emph{hardly}.
The licensing element can also occur in a higher clause, in which case it may be a verb like \emph{doubt}.
%
\begin{exe}
    \ex
    \begin{xlist}
        \ex[]   {Nobody believes you did so much as lift a finger.}
        \ex[*]  {I believe you did so much as lift a finger.}
        \ex[]   {I doubt you did so much as lift a finger.}
    \end{xlist}
\end{exe}

The standard explanation is in term of semantic entailment patterns, with NPIs like \emph{lift a finger} and \emph{ever} restricted to downward entailing patterns, while positive polarity items (PPIs) like \emph{somewhat} are restricted to contexts that are not downward entailing.
The specifics of this proposal are not all that relevant to us except for one crucial property: negation switches downward entailing contexts into non-downward entailing ones, and \emph{vice versa}.
So if an NPI is licensed in configuration $C$, it should not be licensed in $\neg C$, but be licensed again in $\neg \neg C$.
If this is indeed the correct analysis, then the distribution of NPIs and PPIs cannot be defined in first-order logic.
That's because we are dealing with a \emph{modulo} counting pattern, similar to what we saw for the string language $(aa)^+$, and these patterns require the full power of regular languages.
Since first-order logic is a fragment of MSO, it does not have access to this additional power and thus cannot express the semantic generalization.

But whenever one is faced with generalizations that need more power than expected, the first step of action is to verify that \textsc{i}) the generalization is empirically correct, and \textsc{ii}) there isn't a structurally simpler generalization that covers the same data.
In the case of NPI and PPI licensing, the predicted pattern of alternating licensing contexts simply does not obtain.
%
\begin{exe}
    \ex
    \begin{xlist}
        \ex[]  {I doubt you did so much as lift a finger. (predicted grammatical)}
        \ex[?] {I doubt you didn't do so much as lift a finger. (predicted ungrammatical)}
        \ex[?] {Nobody doubts you did so much as lift a finger. (predicted ungrammatical)}
        \ex[??] {Nobody doubts you didn't do so much as lift a finger. (predicted grammatical)}
    \end{xlist}
\end{exe}
%
In addition, NPIs are frequently accepted in contexts that look downward entailing but actually aren't.
%
\begin{exe}
    \ex Every politican that no news paper reported on in great detail has ever made it past the first round.
\end{exe}
%
These effects are usually attributed to processing errors, but in combination with the other facts they cast further doubt on the canonical analysis.
Perhaps the distribution of NPIs and PPIs is subject to a first-order definable constraint after all, a constraint that looks quite a bit different from the semantic generalization simply due to the fact that first-order logic is incapable of expressing such a generalization.

\subsection{Binding Theory}

Binding theory regulates the distribution of elements whose reference depends on some other element in the sentence.
These usually take the form of reflexives (\emph{himself}, \emph{herself}, \emph{itself}), pronouns (\emph{her}, \emph{him}, \emph{it}), and reciprocals (\emph{each other}).
The involvement of reference and meaning turns binding theory into a very wide and vaguely circumscribed phenomenon that spans syntax, semantics, and pragmatics.
How exactly it can and should be factored across these components is difficult to decide, and as we will see next, the FO-syntax conjecture hinges on what kind of factorization one assumes.

The canonical view of binding theory is that DPs and referentially dependent elements are assigned referential indices as in the examples below.
%
\begin{exe}
    \ex
    \begin{xlist}
        \ex[]  {John$_i$ hit himself$_i$.}
        \ex[]  {John$_i$ threatened Bill$_j$ to hit him$_j$.}
        \ex[*] {John$_i$ hit him$_i$.}
        \ex[*] {Mary$_i$ hit himself$_j$.}
    \end{xlist}
\end{exe}
%
Sentences are then filtered out according to whether these indixations satisfy certain principles.
The exact nature of said principles vary across languages, but we can identify to macro-parameters (cf.\ \citealp{Kiparsky02, Kiparsky12}).
%
\begin{description}
    \item[$\pm$ s-bound] Does the referentially dependent element need a \emph{syntactic} antecedent within a certain locality domain? (rather than just a previously established discourse referent)?
    \item[$\pm$ obviative] Must the index of the referentially dependent element be unique within a certain syntactic locality domain?
\end{description}
%
English \emph{himself} for instance, must be s-bound within the smallest TP containing a subject, but it is not obviative.
English \emph{him}, on the other hand, is more ambiguous.

In general, \emph{him} needs no syntactic antecedent, but it does presuppose a previously established discourse referent.
This requirement is lifted when the pronoun is used \emph{deictically}, e.g.\ by pointing at somebody in the room.
Obviation is a little trickier to determine.
The sentence \emph{Gatsby$_i$ is much older than him$_i$} seems perfectly fine when the speaker, mistaken about Gatsby's appearance, is unknowingly pointing at Gatsby.
\Note{See \citet{Heim98} for a detailed discussion of such sentences.}
At the same, it is unclear that this sentence actually involves two identical indices, rather than two distinct indices (= distinct people in the speaker's mind) that happen to map to the same person in the real world.
Excluding these special cases, though, \emph{him} is obviative with the smallest TP containing a subject, which is witnessed by the fact that \emph{John$_i$ hit him$_i$} cannot mean the same thing as \emph{John hit himself}.

Other languages allow for different combinations.
The Icelandic reflexive \emph{sig}, for example, mirrors \emph{himself} in that it is not obviative, but at the same time its locality domain for s-binding is much larger.
Swedish \emph{sig} and Marathi \emph{aapan} have a similar s-binding requirement within a large locality domain but in addition they are obviative within the smallest TP containing a subject.

Given such a tremendous amount of cross-linguistic variation, one might be rightfully worried about the feasibility of a first-order formalization of binding theory.
These worries are justified, but the culprit isn't variation; it is indexation.
By assumption, languages are infinite and the length of sentences is unbounded.
Consequently, there is no upper limit on how many referents may be mentioned in a sentence, which in turn implies that there is no finite bound on the number of indices we need --- for every sentence with one more DP we may have to add one more index.
This is obviously a problem given our assumption that alphabets must be finite, but there are ways to encode indices without putting them in the alphabet --- for example, if all indices are natural numbers then \emph{John$_n$} could be represented as a node \emph{John} whose sister is the root of a unary branching tree of depth $n$.
But \citet{Rogers98} shows that no matter how indices are represented, the free-indexation system at the core of binding theory increases the power of the system so much that it becomes \emph{undecidable}.
That is to say, it is no longer possible to determine for arbitrary, freely indexed trees whether they are well-formed or not.
So binding theory as commonly envisioned isn't just a problem for first-order logic, it is an insurmountable task for all computational machinery.

Of course this formal result seems at odds with our intuition that speakers do not have a hard time determining referents, so either all real-world problems simply happen to be the most trivial instances of an enormously complex system, or free indexation isn't the right way to think about binding theory --- or possibly both.
Index-free theories of binding theory have been developed in the meantime \citep{Bonato05,Kobele06}, defusing the problem to some extent.
%fixme: add Sauerland reference
This is no guarantee, though, that binding theory is first-order definable.
In order to show that, we have to go one step further.

As mentioned at the outset of this section, binding theory combines three different mechanisms:
%
\begin{itemize}
    \item a syntactic mechanism that regulates whether specific lexical items may occur in a given context,
    \item a semantic mechanism that links the interpretation of referentially dependent elements to antecedents in the structure or the discourse,
    \item a discourse mechanism that keeps track of referents and uses pragmatic principles to help in discourse resolution.
\end{itemize}
%
The FO-conjecture is about the complexity of syntax, not semantics or pragmatics.
So only the syntactic part of binding theory can furnish a licit counterexample.
This syntactic part does not have to pay attention to specific meanings, since those are established by semantics.
Instead, it only has to ensure that the distributional requirements are satisfied, which is the case as long as there is at least one possible reading.
In other words, syntax only has to rule out sentences where the referentially dependent elements are distributed in such a fashion that it is impossible to assign a meaning according to the laws of binding theory.

With this shift in perspective, binding theory becomes a lot simpler at the syntactic level.
Determining that s-binding is satisfied isn't all that different from our constraint earlier on that every trace needs to be c-commanded by a mover.
The definition of locality differs quite a bit, and what counts as a valid antecedent is more restricted (\emph{Mary} cannot be an antecedent of \emph{himself} due to gender mismatch), still it all fits comfortably within the bounds of first-order logic.
Obviation is a lot harder.
Due to the unbounded nature of various constructions, it is theoretically possible to have an unbounded number of obviative elements within the same obviation domain.
\Note{%
    This is a necessary condition, not a sufficient one.
    We ignore the extra conditions here for the sake of exposition.
    See \citet{GrafAbner12TAG} for a full discussion.
}
Now it necessarily holds that if there are $n$ obviative elements within the same obviation domain, then there must be at least $n$ possible antecedents outside the obviation domain.
You might remember that this kind of unbounded counting cannot be achieved with finite-state methods, which are equivalent to MSO, which subsumes first-order logic.
So first-order logic cannot even handle syntactic binding theory unless the following additional restriction holds in every natural language:
%
\begin{description}
    \item[Limited Obviation] There is an upper bound $k$ such that if an obviation domain contains $n > k$ obviative elements, $k$ antecedents suffice to furnish a grammatical reading.
\end{description}
%
Limited Obviation thus posits that some obviative elements within the same obviation domain can have the same referent, and that only a fixed number of antecedents are ever needed thanks to this potential overlap.

Limited Obviation is not the kind of condition linguists would come up with, it follows strictly from our search for a restriction that renders binding theory first-order definable.
This makes it even more surprising that Limited Obviation seems to be an actual language universal, as is argued by \citet{GrafAbner12TAG} based on a careful examination of a variety of unbounded constructions.
So even though binding theory is a tight fit, it does stay within the bounds of first-order logic.

\subsection{Copy Movement}

\section{Towards a Context-Free Solution}
