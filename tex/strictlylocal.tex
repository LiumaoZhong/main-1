\chapter{Strictly Local Dependencies}
\label{cha:SL}

\begin{itemize}
    \item idea: following word-final sequences are well-formed
    \item bigrams
    \item bigram grammars
    \item language of a bigram grammar
    \item but we could just as well have list of ill-formed sequences; more in line with current thinking; world's smallest non-trivial tableau here
    \item languages of positive and negative bigram grammars
    \item are the two equivalent? yes
    \item = proof in terms of subset relations
    \item closure under complementation
    \item think of grammars as constraints/dependencies; can we combine dependencies? yes, closure under intersection
    \item but: no closure under union ($\setof{ab} \cup \setof{b, bb, bbb, \ldots}$)
    \item exercise: show that closure under union can fail even if the two languages are both infinite or both finite
    \item no closure under relabeling ( b->a; (ab)* -> (aa)*);
    \item characterization in terms of substring substitution closure
    \item learning bigram languages; lattices
    \item generalization to n-grams; many follow-up exercises
    \item cognitive and learnability implications for local dependencies
\end{itemize}

\section{Phonological Processes as Lists}
Let's return to the specific phenomenon of final devoicing for a moment.
This process can be captured by straying not too far from our idea of phonology as a list, but without running into the issues the latter faces.
In order to verify that a word satisfies word-final devoicing, it suffices to look at the very last allophone;
the word is phonologycally well-formed only if the last sound is not voiced.
Since every language has only a finite number of allophones, we can write a list of allophones that may occur in word-final position, and those that may not.
%
\begin{center}
    \begin{tabular}{r@{\hskip 2em}ccccc}
        \textbf{word-final}     & p & t & k & s & \ldots\\
        \textbf{not word-final} & b & d & g & z & \ldots
    \end{tabular}
\end{center}
%
This list will vary between languages, but it fully characterizes which sounds may occur at the end of a word.

But final devoicing is only one of myriads of local processes in phonology, so unless the idea above can easily be generalized to other local processes it is of very little practical use.
Remember, treating phonology as a list of word pronunciations may not be elegant or enlightening, but at least it is guaranteed to always yield correct output forms.

Consider a simple process like nasal assimilation in English, where a nasal consonant (\textipa{m}, \textipa{n}, \textipa{\ng}) that is followed by a plosive (\textipa{p}, \textipa{b}, \textipa{t}, \textipa{d}, \textipa{k}, \textipa{g}) changes into the nasal whose place of articulation is closest to that of the plosive.
For instance, \textipa{/b\ae nk/} is pronounced \textipa{b\ae\ng k} --- the alveolar \textipa{n} turns into velar \textipa{\ng} by virtue of \textipa{k} being velar.
In this case we can also write a list, but we need more than two categories.
%
\begin{center}
    \begin{tabular}{r@{\hskip 2em}c}
        \textbf{before \textipa{p} or \textipa{b}} & \textipa{m}\\
        \textbf{before \textipa{t} or \textipa{d}} & \textipa{n}\\
        \textbf{before \textipa{k} or \textipa{g}} & \textipa{\ng}\\
    \end{tabular}
\end{center}
%
This list is not quite correct however, because plosives can be preceded by sounds besides nasals.
So a more accurate listing would also need to contain vowels and other consonants.
%
\begin{center}
    \begin{tabular}{r@{\hskip 2em}cccc}
        \textbf{before \textipa{p} or \textipa{b}} & \textipa{m}   & \textipa{a} & \textipa{s} & \ldots\\
        \textbf{before \textipa{t} or \textipa{d}} & \textipa{n}   & \textipa{a} & \textipa{s} & \ldots\\
        \textbf{before \textipa{k} or \textipa{g}} & \textipa{\ng} & \textipa{a} & \textipa{s} & \ldots\\
    \end{tabular}
\end{center}

It looks like we now have list-based accounts for two distinct processes, but how can we be sure that the two are actually similar accounts?
For instance, the following list should strike you as a lot stranger than the previous two.
%
\begin{center}
    \begin{tabular}{r@{\hskip 2em}ccccc}
        \textbf{after a perfect number of sounds}     & \textipa{p} & \textipa{t} & \textipa{k} & \textipa{z} & \ldots\\
        \textbf{after an imperfect number of sounds} & \textipa{b} & \textipa{d} & \textipa{g} & \textipa{s} & \ldots\\
    \end{tabular}
\end{center}
%
A \emph{perfect number} is a number that is identical to the sum of its remainder-free divisors, e.g.\ $6 = 1 + 2 + 3$ or $28 = 1 + 2 + 4 + 7 + 14$.
The list above, then, tells us that certain consonants may be voiced iff their position in the word is a perfect number.
Of course no natural language behaves like that.
Consequently we need an explanation as to why the first two kinds of lists are phonologically natural, whereas the third one is not.

The most obvious answer is that there is a difference in how these lists pick out positions.
The first two regulate the shape of a sound depending on what it is followed by.
The nasalization process can easily be compiled out into a list of well-formed sequences of two sounds: \textipa{mp}, \textipa{mb}, \textipa{nt}, \textipa{nd}, \textipa{\ng k}, \textipa{\ng g}, \textipa{ap}, \textipa{ab}, and so on.
We call such sequences of two sounds \emph{bigrams}.
Crucially the list does not contain bigrams like \textipa{nk}, so a word where \textipa{n} is followed by \textipa{k} is not phonologically well-formed.
The same compiling-out procedure can be used for word-final devoicing if we use the special symbol $\RightEdgeSymbol$ to mark the end of the word.
The list contains the bigrams \textipa{p\RightEdge}, \textipa{t\RightEdge}, \textipa{k\RightEdge}, \textipa{s\RightEdge}, but not \textipa{b\RightEdge}, \textipa{d\RightEdge}, \textipa{g\RightEdge}, or \textipa{z\RightEdge}.
Thus the first two processes share the property that they can be described via a finite list of bigrams.

The hypothetical perfect-number phenomenon, on the other hand, cannot be described in terms of bigrams.
A sequence like \textipa{ap} is licit in the string XXX, but illicit in \textipa{tRap}. %fixme: example string
The way sounds are regulated no longer depends on adjacency to a specific sound but rather a highly abstract counting mechanism.
So one idea we may explore for now is that all phonological processes can be captured via finite lists of bigrams.
In other words, phonology is a bigram grammar.

\section{Bigram Grammars and Recognizers}

A \emph{bigram grammar} is a just finite set of bigrams.
Each bigram encodes a licit sequence of two sounds.
So a bigram grammar $G$ considers a string $w$ well-formed iff every sequence of two adjacent symbols in $w$ is a bigram of $G$.
We refer to these sequences as bigrams, too.
In other words, a string is well-formed iff it consists only of bigrams that are licensed by the grammar.
We also say that the bigram grammar \emph{generates} this string.

\begin{examplebox}[Strings licensed by a small bigram grammar]
    \label{ex:SL_BigramGrammar}%
    Suppose grammar $G$ contains the bigrams $\LeftEdge a$, $\mathit{ab}$, $b \RightEdge$, where $\LeftEdgeSymbol$ and $\RightEdgeSymbol$ mark the left and right edge of the string, respectively.
    Now consider the string $\mathit{ab}$, which we may also write $\LeftEdge \mathit{ab} \RightEdge$ for the sake of explicitness.
    This string consists of the bigrams $\LeftEdge a$, $\mathit{ab}$, and $b \RightEdge$.
    Since all of those bigrams are part of $G$, the string is well-formed. 

    The minimally different $\mathit{ba}$, on the other hand, is built from the bigrams $\LeftEdge b$, $\mathit{ba}$, and $\mathit{a \RightEdge}$.
    Not a single one of those bigrams is contained in $G$, and consequently $G$ does not generate this string.

    The string $\mathit{abb}$ is not generated by $G$ either.
    Even though three of its bigrams are part of $G$ --- namely $\LeftEdge a$, $\mathit{ab}$, and $b \RightEdge$ --- its fourth bigram $\mathit{bb}$ is not among $G$'s bigrams.
    So even though 75\% of this string's bigrams are well-formed, it is still not generated by the grammar.
    A single mistake is enough to render it illicit.
\end{examplebox}

A subtle point that is easy to lose sight of is that the grammar is just a specification of which strings are well-formed, it does not include a procedure for verifying whether a given string is actually well-formed.
That is easy to see if we think about how we would implement a bigram grammar in Python.
Given the definition above, a bigram grammar is simply a set of bigrams.
This is straight-forwardly translated into Python.
%
\pythonfile[firstline=3,lastline=3]{./code/bigram_scanner/bigram_scanner_set_unsafe.py}
%
Clearly the definition of a variable is not enough to produce a useful program.
What we need is a mechanism that takes the grammar as a parameter and then determines if a given string is generated by the grammar.
Such a mechanism is called a \emph{recognizer} as it recognizes whether a string is well-formed.

There's many different ways one may construct a recognizer, but for a bigram grammar the simplest model is that of a \emph{scanner}.
The scanner has a window that is exactly two symbols wide.
It moves this window through the string from left to right, and at each step it checks whether the sequence of symbols it sees in its window is part of the bigram grammar (see Fig.~\ref{fig:SL_Scanner} for an illustration).
If the answer is no, it rejects the string.
Otherwise it moves the window one symbol further to the right.
If the window cannot be moved further to the right, the scanner accepts the string as well-formed.
%
\begin{figure}[tbph]
    
\caption{Scanner for the bigram grammar from example~\ref{ex:SL_BigramGrammar}}
\label{fig:SL_Scanner}
\end{figure}

\pythonfile[firstline=5]{./code/bigram_scanner/bigram_scanner_set_unsafe.py}

\begin{center}
    \begin{tabular}{r|c}
        \textipa{/rad/} & $^*$[+voice]$\RightEdge$\\
        \hline
        \textipa{[rad]} & !* \\
        \textipa{[rat]} &
    \end{tabular}
\end{center}

\begin{definition}[Bigrams]
    Given a string $w$ over alphabet $\Sigma$, its \emph{augmented} counterpart $\augmented{w} \is \LeftEdge \stringcat w \stringcat \RightEdge$ is obtained by adding the left and right edge markers $\LeftEdge$ and $\RightEdge$ to $w$, where $\LeftEdge, \RightEdge \notin \Sigma$.
    Furthermore, 
    \(
    \Bigrams(w) \is
        \setof{
            ab \mid \exists u,v  \text{ s.t.\ } u \stringcat ab \stringcat v = \augmented{w}
        }
    \)
    denotes the set of \emph{bigrams} over $\augmented{w}$, i.e.\ the smallest set that contains all substrings of $\augmented{w}$ that consist of exactly $2$ symbols.
\end{definition}

\begin{definition}[Bigram Grammar]
    A \emph{bigram grammar} $G$ over alphabet $\Sigma$ is a finite set of bigrams over $\Sigma \cup \setof{\LeftEdge, \RightEdge}$.
    If $G$ is a \emph{positive bigram grammar} (denoted $\posG{G}$), then it generates the language
    \(
        L(G) \is \setof{
            w \mid \Bigrams(w) \subseteq G
        }
    \).
    If $G$ is a \emph{negative bigram grammar} (denoted $\negG{G}$), then it generates the language
    \(
        L(G) \is \setof{
            w \mid \Bigrams(w) \cap G = \emptyset
        }
    \).
\end{definition}

\begin{theorem}
    The class of languages that are generated by positive bigram grammars is exactly the class of languages that are generated by negative bigram grammars.
\end{theorem}

\begin{lemma}
    For every positive bigram grammar there is a negative bigram grammar that generates the same language.
\end{lemma}
%
\begin{proof}
    Let $\posG{G}$ be a positive bigram grammar. 
    We show that $\negG{\complementof{G}}$ defines the same language as $\posG{G}$, where $\complementof{G}$ consists of all bigrams over $\Sigma \cup \setof{\LeftEdge, \RightEdge}$ that are not contained by $G$.
    Pick some arbitrary string $w \in L(\posG{G})$ such that $\Bigrams(w) \is \setof{ w_1, \ldots, w_n }$, $n \geq 0$.
    By definition, every bigram of $w$ is a member of $G$, which immediately implies that no element of $\Bigrams(w)$ is contained in $\complementof{G}$.
    But if none of the bigrams of $w$ belong to $\complementof{G}$, then $\Bigrams(w) \cap \complementof{G} = \emptyset$, wherefore $w \in L(\negG{\complementof{G}})$.
    Since $w$ was arbitrary, this result holds for every string generated by $\posG{G}$, establishing its equivalence to $\negG{\complementof{G}}$.
\end{proof}

\begin{lemma}
    For every negative bigram grammar there is a positive bigram grammar that generates the same language.
\end{lemma}
%
\begin{proof}
    Left as an exercise to the reader.
\end{proof}


\section{Basic Closure Properties}

\begin{lemma}
    The class of bigram languages is closed under intersection.    
\end{lemma}
%
\begin{proof}
    We prove that for any two bigram languages generated by (positive) bigram grammars $G_1$ and $G_2$, $L(G_1) \cap L(G_2) = L(G_1 \cap G_2)$.
    We first show $L(G_1) \cap L(G_2) \subseteq L(G_1 \cap G_2)$.
    Let $w$ be an arbitrary string belonging to both $L(G_1)$ and $L(G_2)$, i.e.\ $w \in L(G_1) \cap L(G_2)$.
    Then every bigram of $w$ is contained in both $G_1$ and $G_2$, so $w \in L(G_1 \cap G_2)$.
    Since $w$ is arbitrary, we have $L(G_1) \cap L(G_2) \subseteq L(G_1 \cap G_2)$.
    The same reasoning can be applied in the other direction, yielding $L(G_1 \cap G_2) \subseteq L(G_1) \cap L(G_2)$.
    These two facts jointly imply $L(G_1) \cap L(G_2) = L(G_1 \cap G_2)$.
\end{proof}
%
% hw: spell out the second subset relation

\begin{lemma}
    The class of bigram languages is not closed under union. 
\end{lemma}
%
\begin{proof}
    We give an example of two bigram languages whose union is not a bigram language.
    Let $L_1 \is \setof{\mathit{ab}}$ and $L_2 \is \setof{\mathit{b}, \mathit{bb}, \mathit{bbb}, \ldots}$.
    Assume without loss of generality (w.l.o.g.) that all bigram grammars are positive.
    Then any bigram grammar that generates $L_1$ must contain the bigrams $\LeftEdge a$, $\mathit{ab}$, and $b \RightEdge$.
    Similarly, a bigram grammar generating $L_2$ must contain the bigrams $\LeftEdge b$, $\mathit{bb}$, and $b \RightEdge$.
    Therefore a bigram grammar that generates all strings in $L_1 \cup L_2$ must contain at least these bigrams.
    But such a grammar also generates the string $\mathit{abb}$, which is not part of $L_1 \cup L_2$.
    Hence there is no bigram grammar that generates all the strings in $L_1 \cup L_2$ and nothing else, so $L_1 \cup L_2$ is not a bigram language.
\end{proof}
%
%hw: extend proof to case where both languages are infinite

\begin{lemma}
    The class of bigram languages is not closed under (relative) complement.
\end{lemma}
%
\begin{proof}
    This is an immediate consequence of the previous lemmata and De Morgan's law: $L_1 \cup L_2 = \complementof{\complementof{L_1} \cap \complementof{L_2}}$.
    If the class of bigram languages were closed under both complement and intersection, it would be closed under union, too.
    Since closure under union does not hold, closure under intersection or complement must fail.
    We have already seen that closure under intersection holds, so closure under complement cannot hold, too.
\end{proof}
%
%hw: give a proof by example
Slightly surprising because we have the option to take complements of grammars, but that's not string language complements.

\begin{lemma}
    The class of bigram languages is not closed under relabelings.
\end{lemma}
%
\begin{proof}
    Consider the language $L \is \setof{\mathit{ab}, \mathit{abab}, \mathit{ababab}, \ldots}$ and the relabeling $r$ that replaces all $b$s by $a$s.
    The image of $L$ under $r$ consists of all strings that contain a (positive) even number of $a$s and nothing else: $\mathit{aa}$, $\mathit{aaaa}$, $\mathit{aaaaaa}$, and so on.
    Every such string contains the bigrams $\LeftEdge a$, $\mathit{aa}$, and $a \RightEdge$.
    But a grammar with these bigrams also generates the strings $a$, $\mathit{aaa}$, $\mathit{aaaaa}$, and so on, which shows that the image of $L$ under $r$ cannot be generated by a bigram grammar.
    Consequently, closure under relabelings does not hold for the class of bigram languages.
\end{proof}
%
ties into abstractness debate (e.g. if we have hidden C-V layer, language is definable).

%hw: implement a negative bigram grammar
%hw: implement translation from positive into negative bigram grammar

local substring substitution

%hw: what happens if we run our scanner with grammar for {LR} and string LLLRRR; is this accepted? why? how to fix the issue?
